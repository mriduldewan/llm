{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Phi3 and Langchain MLXPipeline\n",
    "\n",
    "This code establishes a chat model powered by the Phi-3 large language model (LLM) using the LangChain library. It leverages the MLX Pipeline to interact with the Phi-3 model and enables conversation-like interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b612b5a82a24fe98f8caef08432afb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a050e3ea6b248c6975c953a943263ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83404fae49e4db8a3d274df04a7f57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54871e7c80c48d2a048731ce5d1aad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bd9e0025ff4253b55f3b65397de8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab5429352804865a847a7ab04d6fce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/9.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628f57996ff941819641e60abd7bb0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96c9e2df8e74e5799f311760cd385cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sample_finetune.py:   0%|          | 0.00/3.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11381a8f58d648188383f3dda2b181c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafd6a0b5cdf4ae294fa4db1d97775a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] rope_scaling 'type' currently only supports 'linear' setting rope scaling to false.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Import MLX Pipeline class\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "\n",
    "# Create MLX Pipeline instance for \"Phi-3-mini-128k-instruct-8bit\" model\n",
    "llm = MLXPipeline.from_model_id(\n",
    "    \"mlx-community/Phi-3-mini-128k-instruct-8bit\",\n",
    "\n",
    "    # Set maximum output length to 100 tokens\n",
    "    pipeline_kwargs={\"max_tokens\": 100},\n",
    "\n",
    "    # Set temperature parameter for less randomness in generation\n",
    "    temp=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HumanMessage class for representing human user messages\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Import ChatMLX class for building chatbots using MLX models\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "\n",
    "# Create a ChatMLX instance (chat model) using the previously defined llm (MLX Pipeline)\n",
    "chat_model = ChatMLX(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><|user|>\\nHow many known breeds of dogs are there?<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Message to be sent to the llm\n",
    "query=\"How many known breeds of dogs are there?\"\n",
    "\n",
    "# Create a list to store the message\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=query\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Convert the human message to a format suitable for the chat model (internal method)\n",
    "chat_model._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in 2023, there are over 340 recognized breeds of dogs according to the Fédération Cynologique Internationale (FCI). However, the number of recognized breeds can vary slightly depending on the organization and the criteria they use for recognition.<|end|><|assistant|> It's important to note that new breeds can be recognized over time, and the number of recognized breeds can change.<|end|><|assistant|> As of my last update, the recognized dog\n"
     ]
    }
   ],
   "source": [
    "# Call the chat model to process and respond to the message(s)\n",
    "res = chat_model.invoke(messages)\n",
    "\n",
    "# Print the response content (generated by the LLM)\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
