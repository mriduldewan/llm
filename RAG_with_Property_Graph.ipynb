{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device variable for cude if available, else using standard CPU\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu' # This is for windows\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # For M1 Mac\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# This will backback to CPU since the llama index is not supported by MPS for llama 3.1\n",
    "#%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 5\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# these expect to find a .env file at the directory above the lesson.                                                                                                                     # the format for that file is (without the comment)                                                                                                                                       #API_KEYNAME=AStringThatIsTheLongAPIKeyFromSomeService                                                                                                                                     \n",
    "def load_env():\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "\n",
    "def get_HF_key():\n",
    "    load_env()\n",
    "    hf_api_key = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "    return hf_api_key\n",
    "\n",
    "HF_TOKEN =  get_HF_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter runs sync in the background, but we need async capabilities for modules in this code\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Create the embeddings\n",
    "embedding_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Create the embeddings\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Model already exists in the directory. Loading from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851fd1e709cd4cfa9f83610350b00e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Tokenizer already exists in the directory. Loading from local directory\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model name that we want to load from HF\n",
    "#model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Setup the cache directory for the model and tokenizer.\n",
    "#cache_dir = \"./model/llama3.1_8b/\"\n",
    "cache_dir = \"./model/llama3_8b/\"\n",
    "\n",
    "# Check if the model is already exists. If it does, load the model. \n",
    "# If file does not exists, then download from HuggingFace\n",
    "model_files = glob.glob(cache_dir + \"*.safetensors\")\n",
    "tokenizer_files = glob.glob(cache_dir + \"tokenizer.json\")\n",
    "\n",
    "if len(model_files)>0:\n",
    "    print(\"--Model already exists in the directory. Loading from local directory\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(cache_dir)\n",
    "\n",
    "else:\n",
    "    # Load the model and the tokenizer\n",
    "    # The model is loaded with the specified quantization configuration\n",
    "    # and the \"auto\" device mapping for efficient inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16, # Original precision is float32, but we will convert to float16 for efficiency. MPS doesnt support bfloat16 so changing to float16\n",
    "            device_map=\"auto\",\n",
    "            token=HF_TOKEN)\n",
    "\n",
    "\n",
    "if len(tokenizer_files)>0:\n",
    "    print(\"\\n--Tokenizer already exists in the directory. Loading from local directory\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cache_dir)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            token=HF_TOKEN)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mridul/anaconda3/envs/llamaindex/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/mridul/anaconda3/envs/llamaindex/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/mridul/anaconda3/envs/llamaindex/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/mridul/anaconda3/envs/llamaindex/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/mridul/anaconda3/envs/llamaindex/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PropertyGraphIndex\n",
    "\n",
    "The following steps occur during the creation of a PropertyGraph:\n",
    "\n",
    "- PropertyGraphIndex.from_documents(): We load documents into an index.\n",
    "\n",
    "- Parsing Nodes: The index parses the documents into nodes.\n",
    "\n",
    "- Extracting Paths from Text: The nodes are passed to an LLM, which is prompted to generate knowledge graph triples (i.e., paths).\n",
    "\n",
    "- Extracting Implicit Paths: The node.relationships property is used to infer implicit paths.\n",
    "\n",
    "- Generating Embeddings: Embeddings are generated for each text node and graph node, occurring twice during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debfbb89c4a84506b27d21fe3a579df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text:   0%|          | 0/71 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Extracting paths from text: 100%|██████████| 71/71 [28:30:55<00:00, 1445.85s/it]      \n",
      "Extracting implicit paths: 100%|██████████| 71/71 [00:00<00:00, 70003.66it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:05<00:00,  1.56it/s]\n",
      "Generating embeddings: 100%|██████████| 168/168 [00:38<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.property_graph_store.save_networkx_graph(name=\"./kg.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure settings\n",
    "Settings.chunk_size = 1024\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "Querying a property graph index typically involves using one or more sub-retrievers and combining their results. The process of graph retrieval includes:\n",
    "\n",
    "Selecting Nodes: Identifying the initial nodes of interest within the graph.\n",
    "Traversing: Moving from the selected nodes to explore connected elements.\n",
    "By default, two primary types of retrieval are employed simultaneously:\n",
    "\n",
    "- Synonym/Keyword Expansion: Utilizing an LLM to generate synonyms and keywords derived from the query.\n",
    "\n",
    "- Vector Retrieval: Employing embeddings to locate nodes within your graph.\n",
    "\n",
    "Once nodes are identified, you can choose to:\n",
    "\n",
    "- Return Paths: Provide the paths adjacent to the selected nodes, typically in the form of triples.\n",
    "\n",
    "- Return Paths and Source Text: Provide both the paths and the original source text of the chunk, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viaweb -> Charged -> $300 a month for a big store\n",
      "Viaweb -> Has -> 70 stores\n",
      "Viaweb -> Developed -> Users control through browsers\n",
      "Yahoo -> Bought -> Viaweb\n",
      "Viaweb -> Grows -> 7x a year\n",
      "Viaweb -> Impact -> Elimination of client software\n",
      "Viaweb -> Had about 500 -> Stores at the end of 1997\n",
      "Viaweb -> Developed -> Software running on server\n",
      "Viaweb -> Founded -> 1995\n",
      "Viaweb -> Was -> Profitable\n",
      "Viaweb -> Sold for -> $2 million a month\n",
      "Viaweb -> Priced at $300 a month -> Big store\n",
      "Viaweb -> Has -> Growth rate\n",
      "Viaweb -> Went through -> Near-death experiences\n",
      "Viaweb -> Impact -> Update software right on server\n",
      "Viaweb -> Was -> Running company\n",
      "Viaweb -> Impact -> Elimination of typing into command line on server\n",
      "Viaweb -> Founded -> 1982\n",
      "Viaweb -> Is -> Startup\n",
      "Viaweb -> Low price -> Due to author's lack of knowledge\n",
      "Viaweb -> Grew -> 7x a year\n",
      "Viaweb -> Low price -> Competitive advantage\n",
      "Paul graham -> Was -> Partner\n",
      "Viaweb -> Had about 70 -> Stores at the end of 1996\n",
      "Viaweb -> Low price -> Challenging for competitors\n",
      "Viaweb -> Was -> Inexpensive\n",
      "Viaweb -> Priced at $100 a month -> Small store\n",
      "Viaweb -> Low price -> Attractive to customers\n",
      "Viaweb -> Impact -> Development of new generation of software\n",
      "Viaweb -> Was -> New company\n",
      "Author -> Lack of business knowledge -> Contributed to the success of viaweb\n",
      "(author -> Role in -> Interleaf\n",
      "Interleaf -> Added -> A scripting language\n",
      "Interleaf -> Was -> The author's workplace\n",
      "Interleaf -> Scripting language -> Was a dialect of lisp\n",
      "Interleaf -> Still -> Had a few years to live\n",
      "Interleaf -> Added -> Lisp\n",
      "Interleaf -> Added -> Scripting language\n",
      "Interleaf -> Wanted -> Lisp hacker\n",
      "Emacs -> Inspired -> Interleaf's scripting language\n",
      "The author -> Was worn out from -> Effort and stress of running viaweb\n"
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever(\n",
    "    include_text=False,  # include source text, default True\n",
    ")\n",
    "\n",
    "nodes = retriever.retrieve(\"What happened at Interleaf and Viaweb?\")\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " The author experimented with a new kind of still life painting after moving back to New York. This represents the author's creative adaptation to his new circumstances, as he was unable to find fulfillment in his new life in California and was desperate to refocus on his passion for painting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    include_text=True\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What happened at Interleaf and Viaweb?\")\n",
    "\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
