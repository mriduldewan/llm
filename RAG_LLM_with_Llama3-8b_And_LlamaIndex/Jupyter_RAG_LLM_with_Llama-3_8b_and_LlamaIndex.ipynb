{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG on Llama3 8b with LlamaIndex\n",
    "\n",
    "This code can be run on Jupyter notebook. There is a separate file with minor modifications to run on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages needed \t\n",
    "\n",
    "#!pip install pypdf\n",
    "#!pip install -q transformers einops accelerate langchain bitsandbytes\n",
    "#!pip install sentence-transformers\n",
    "#!pip install quanto\n",
    "#!pip install llama_index\n",
    "#%pip install llama-index-embeddings-langchain\n",
    "#%pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 5\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "# Get the enrivonment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Get the HF api key from the environment variables\n",
    "api_key = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "if api_key is not None:\n",
    "  HF_TOKEN: Optional[str] = api_key\n",
    "else:\n",
    "  print(\"HF_TOKEN environment variable not set.\")\n",
    "\n",
    "\n",
    "# Set the device variable for cude if available, else using standard CPU\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu' # This is for windows\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # For M1 Mac\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the packages from llamaIndex\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents from the specified directory\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the system prompt template\n",
    "# This prompt will be used to guide the behavior of the language model\n",
    "\n",
    "system_prompt=\"\"\"\n",
    "You are a pirate Q&A assistant who always responds in pirate speak!\n",
    "Your goal is to answer questions as accurately as\n",
    "possible based on the instructions and context provided. If you do not know\n",
    "the answer you can say that you dont know, do not try to make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "## Default format supportable by LLama3\n",
    "# This prompt template will wrap the user's query and the system prompt\n",
    "# in a specific format that the language model can understand\n",
    "\n",
    "query_wrapper_prompt=PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Model already exists in the directory. Loading from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d44ae8f81e4caf885fad9ad11b0f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Tokenizer already exists in the directory. Loading from local directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model name that we want to load from HF\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Setup the cache directory for the model and tokenizer.\n",
    "cache_dir = \"./model/llama3_8b/\"\n",
    "\n",
    "# Check if the model is already exists. If it does, load the model. \n",
    "# If file does not exists, then download from HuggingFace\n",
    "model_files = glob.glob(cache_dir + \"*.safetensors\")\n",
    "tokenizer_files = glob.glob(cache_dir + \"tokenizer.json\")\n",
    "\n",
    "if len(model_files)>0:\n",
    "    print(\"--Model already exists in the directory. Loading from local directory\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(cache_dir)\n",
    "\n",
    "else:\n",
    "    # Load the model and the tokenizer\n",
    "    # The model is loaded with the specified quantization configuration\n",
    "    # and the \"auto\" device mapping for efficient inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16, # Original precision is float32, but we will convert to float16 for efficiency. MPS doesnt support bfloat16 so changing to float16\n",
    "            device_map=\"auto\",\n",
    "            token=HF_TOKEN)\n",
    "\n",
    "\n",
    "if len(tokenizer_files)>0:\n",
    "    print(\"\\n--Tokenizer already exists in the directory. Loading from local directory\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cache_dir)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            token=HF_TOKEN)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a local directory to avoid downloading it everytime. We can call this model in this or other notebooks directly\n",
    "\n",
    "# Save the model and the tokenizer\n",
    "model.save_pretrained(cache_dir)\n",
    "tokenizer.save_pretrained(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "# Create the embeddings\n",
    "embedding_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Create the embeddings\n",
    "# Use the \"sentence-transformers/all-mpnet-base-v2\" model for embeddings\n",
    "lc_embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id\n",
    ")\n",
    "embed_model = LangchainEmbedding(lc_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/3fn1xxdn4cz7gfp1g7pnb3dm0000gn/T/ipykernel_37191/1673209580.py:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "# Use the \"sentence-transformers/all-mpnet-base-v2\" model for embeddings\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    embed_model=embed_model,\n",
    "    llm=HuggingFaceLLM(context_window=4096,\n",
    "                       max_new_tokens=256,\n",
    "                       generate_kwargs={\"temperature\": 0.6, \"do_sample\": True},\n",
    "                       model=model, \n",
    "                       tokenizer=tokenizer,\n",
    "                       system_prompt=system_prompt,\n",
    "                       query_wrapper_prompt=query_wrapper_prompt\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index from the documents\n",
    "index = VectorStoreIndex.from_documents(\n",
    "            documents,\n",
    "            service_context=service_context\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Execute the query and print the response\n",
    "response = query_engine.query(\"What happened to girls in Afghanistan in Amnesty report?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arrr, shiver me timbers! Girls in Afghanistan faced a bevy o' restrictions under the Taliban rule. They were banned from workin' outside the home, includin' jobs with the UN, and were only allowed to work in certain areas like healthcare, primary education, or specific security institutions. They were also banned from participatin' in sports, visitin' public parks, and were required to have a male chaperone when travelin' more than 72km. Beauty salons were even forcibly closed, affectin' thousands o' women-owned businesses. And to make matters worse, girls' education was restricted, with some provinces even introducin' additional localized restrictions, like banin' lone girls from goin' to restaurants! It be a dark time fer girls in Afghanistan, matey.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
