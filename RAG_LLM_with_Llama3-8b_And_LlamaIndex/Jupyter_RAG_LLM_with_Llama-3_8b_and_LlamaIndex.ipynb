{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "#!pip install pypdf\n",
    "#!pip install -q transformers einops accelerate langchain bitsandbytes\n",
    "#!pip install sentence-transformers\n",
    "#!pip install quanto\n",
    "#!pip install llama_index\n",
    "#%pip install llama-index-embeddings-langchain\n",
    "#%pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 5\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "if api_key is not None:\n",
    "  HF_TOKEN: Optional[str] = api_key\n",
    "else:\n",
    "  print(\"HF_TOKEN environment variable not set.\")\n",
    "\n",
    "\n",
    "# Set the device variable for cude if available, else using standard CPU\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu' # This is for windows\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # For M1 Mac\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents from the specified directory\n",
    "\n",
    "#documents = SimpleDirectoryReader(\"/content/data\").load_data() # Path if working in colab\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the system prompt template\n",
    "# This prompt will be used to guide the behavior of the language model\n",
    "\n",
    "system_prompt=\"\"\"\n",
    "You are a pirate Q&A assistant who always responds in pirate speak!\n",
    "Your goal is to answer questions as accurately as\n",
    "possible based on the instructions and context provided. If you do not know\n",
    "the answer you can say that you dont know, do not try to make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "## Default format supportable by LLama3\n",
    "# This prompt template will wrap the user's query and the system prompt\n",
    "# in a specific format that the language model can understand\n",
    "\n",
    "query_wrapper_prompt=PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Model already exists in the directory. Loading from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e232726895c74553b4070cbade1103c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Tokenizer already exists in the directory. Loading from local directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model name that we want to load from HF\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Setup the cache directory for the model and tokenizer.\n",
    "cache_dir = \"./model/llama3_8b/\"\n",
    "\n",
    "# Check if the model is already exists. If it does, load the model. \n",
    "# If file does not exists, then download from HuggingFace\n",
    "model_files = glob.glob(cache_dir + \"*.safetensors\")\n",
    "tokenizer_files = glob.glob(cache_dir + \"tokenizer.json\")\n",
    "\n",
    "if len(model_files)>0:\n",
    "    print(\"--Model already exists in the directory. Loading from local directory\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(cache_dir)\n",
    "\n",
    "else:\n",
    "    # Load the model and the tokenizer\n",
    "    # The model is loaded with the specified quantization configuration\n",
    "    # and the \"auto\" device mapping for efficient inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16, # Original precision is float32, but we will convert to float16 for efficiency. MPS doesnt support bfloat16 so changing to float16\n",
    "            device_map=\"auto\",\n",
    "            token=HF_TOKEN)\n",
    "\n",
    "\n",
    "if len(tokenizer_files)>0:\n",
    "    print(\"\\n--Tokenizer already exists in the directory. Loading from local directory\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cache_dir)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            token=HF_TOKEN)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a local directory to avoid downloading it everytime. We can call this model in this or other notebooks directly\n",
    "\n",
    "# Save the model and the tokenizer\n",
    "model.save_pretrained(cache_dir)\n",
    "tokenizer.save_pretrained(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "# Create the embeddings\n",
    "embedding_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Create the embeddings\n",
    "# Use the \"sentence-transformers/all-mpnet-base-v2\" model for embeddings\n",
    "lc_embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id\n",
    ")\n",
    "embed_model = LangchainEmbedding(lc_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"sentence-transformers/all-mpnet-base-v2\" model for embeddings\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    embed_model=embed_model,\n",
    "    llm=HuggingFaceLLM(context_window=4096,\n",
    "                       max_new_tokens=256,\n",
    "                       generate_kwargs={\"temperature\": 0.3, \"do_sample\": False},\n",
    "                       model=model, \n",
    "                       tokenizer=tokenizer,\n",
    "                       system_prompt=system_prompt,\n",
    "                       query_wrapper_prompt=query_wrapper_prompt\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index from the documents\n",
    "index = VectorStoreIndex.from_documents(\n",
    "            documents,\n",
    "            service_context=service_context\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "print(\"Generating Input ID's and applying chat template\")\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "print(\"Generating Output\")\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(\"\\n\\n Response:\")\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
